{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this segmentation is a sub-segmentation of the original segmentations provided by the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) create a dataset of syllable waveforms for each isolation segment\n",
    "\n",
    "2) get segmented times for each syllable in that segment\n",
    "\n",
    "3) create a new dataframe with the isolated syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:01:38.542798Z",
     "start_time": "2019-10-09T03:01:29.852671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:40:59.676284Z",
     "start_time": "2019-10-09T04:40:59.550834Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311972/2400570142.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2023-04-25 22:55:40.197875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 22:55:42.274083: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 22:55:42.274169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 22:55:42.274179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.autonotebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import umap\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:02:11.601790Z",
     "start_time": "2019-10-09T03:02:03.363213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n"
     ]
    }
   ],
   "source": [
    "from avgn.utils.paths import DATA_DIR, most_recent_subdirectory, ensure_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:02:11.646846Z",
     "start_time": "2019-10-09T03:02:11.605916Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'fruitbat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:41:00.835154Z",
     "start_time": "2019-10-09T04:41:00.723277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-25_22-55-44'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique datetime identifier for the files output by this notebook\n",
    "DT_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "DT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:02:11.892201Z",
     "start_time": "2019-10-09T03:02:11.649716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from avgn.utils.hparams import HParams\n",
    "from avgn.dataset import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:02:12.034398Z",
     "start_time": "2019-10-09T03:02:11.896260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from avgn.signalprocessing.create_spectrogram_dataset import prepare_wav, create_label_df, get_row_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:04:36.230519Z",
     "start_time": "2019-10-09T03:04:32.287118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hparams = HParams(\n",
    "    num_mel_bins = 32,\n",
    "    mel_lower_edge_hertz=500,\n",
    "    mel_upper_edge_hertz=120000,\n",
    "    butter_lowcut = 500,\n",
    "    butter_highcut = 120000,\n",
    "    ref_level_db = 20,\n",
    "    min_level_db = -60,\n",
    "    mask_spec = True,\n",
    "    win_length_ms = 0.5,\n",
    "    hop_length_ms = 0.05,\n",
    "    mask_spec_kwargs = {\"spec_thresh\": 0.9, \"offset\": 1e-10},\n",
    "    n_jobs = -1,\n",
    "    verbosity=1,\n",
    "    nex = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:06:24.921555Z",
     "start_time": "2019-10-09T03:04:36.236193Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110ef74bd67d493197280c1e6c68b48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading json:   0%|          | 0/87985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2128 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 57344 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 87985 out of 87985 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "getting unique individuals:   0%|          | 0/87985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 22:55:53.788377: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-25 22:55:53.788445: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-25 22:55:53.789417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create a dataset object\n",
    "dataset = DataSet(DATASET_ID, hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:06:28.448949Z",
     "start_time": "2019-10-09T03:06:24.923602Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.data_files = {i:dataset.data_files[i] for i in list(dataset.data_files.keys())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:06:28.561517Z",
     "start_time": "2019-10-09T03:06:28.455457Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.sample_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "166697\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:06:28.655243Z",
     "start_time": "2019-10-09T03:06:28.565057Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataset.data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset based upon JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:06:28.767630Z",
     "start_time": "2019-10-09T03:06:28.658410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "n_jobs = 4; verbosity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:09:37.108529Z",
     "start_time": "2019-10-09T03:06:28.771615Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663c1d49bc1a46dfa325ac2e268ed581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1671s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0106s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0145s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0274s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0474s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=4)]: Done 216 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0545s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=4)]: Done 520 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1189s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=4)]: Done 1672 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 3336 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done 5256 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done 7176 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 9352 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=4)]: Done 11528 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=4)]: Done 13960 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=4)]: Done 16392 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=4)]: Done 19080 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=4)]: Done 21768 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=4)]: Done 24712 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done 27656 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=4)]: Done 30856 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=4)]: Done 34056 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=4)]: Done 37512 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=4)]: Done 40968 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=4)]: Done 44680 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=4)]: Done 48392 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=4)]: Done 52360 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=4)]: Done 56328 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=4)]: Done 60552 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=4)]: Done 64776 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=4)]: Done 69256 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=4)]: Done 73736 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=4)]: Done 78472 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=4)]: Done 83208 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=4)]: Batch computation too slow (2.0903s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1744s.) Setting batch_size=48.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1002s.) Setting batch_size=96.\n",
      "[Parallel(n_jobs=4)]: Done 86696 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1852s.) Setting batch_size=192.\n",
      "[Parallel(n_jobs=4)]: Done 87848 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=4)]: Done 87985 out of 87985 | elapsed:   37.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91079"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n",
      "PROJECT_DIR set on:  /data0/home/h21/luas6629/Thesis\n"
     ]
    }
   ],
   "source": [
    "with Parallel(n_jobs=n_jobs, verbose=verbosity) as parallel:\n",
    "    syllable_dfs = parallel(\n",
    "        delayed(create_label_df)(\n",
    "            dataset.data_files[key].data,\n",
    "            hparams=dataset.hparams,\n",
    "            labels_to_retain=[\"context\"],\n",
    "            unit=\"syllables\",\n",
    "            dict_features_to_retain = [],\n",
    "            key = key,\n",
    "        )\n",
    "        for key in tqdm(dataset.data_files.keys())\n",
    "    )\n",
    "syllable_df = pd.concat(syllable_dfs)\n",
    "len(syllable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:09:39.976158Z",
     "start_time": "2019-10-09T03:09:37.110880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:09:40.413163Z",
     "start_time": "2019-10-09T03:09:39.979647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syllable_df.context.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:09:40.820122Z",
     "start_time": "2019-10-09T03:09:40.701917Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get only isolation calls\n",
    "#syllable_df = syllable_df[syllable_df.context == \"Isolation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get audio for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = syllable_df.key.unique()[0];\n",
    "get_row_audio(\n",
    "            syllable_df[syllable_df.key == key], \n",
    "            dataset.data_files[key].data['wav_loc'], \n",
    "            dataset.hparams\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:30.467650Z",
     "start_time": "2019-10-09T03:09:40.822493Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_jobs, verbose=verbosity) as parallel:\n",
    "    syllable_dfs = parallel(\n",
    "        delayed(get_row_audio)(\n",
    "            syllable_df[syllable_df.key == key], \n",
    "            dataset.data_files[key].data['wav_loc'], \n",
    "            dataset.hparams\n",
    "        )\n",
    "        for key in tqdm(syllable_df.key.unique())\n",
    "    )\n",
    "syllable_df = pd.concat(syllable_dfs)\n",
    "len(syllable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:34.504442Z",
     "start_time": "2019-10-09T03:11:30.469663Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mask  = np.array([len(i) > 0 for i in tqdm(syllable_df.audio.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:34.603841Z",
     "start_time": "2019-10-09T03:11:34.508741Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df = syllable_df[np.array(df_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:34.710888Z",
     "start_time": "2019-10-09T03:11:34.605887Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:34.803798Z",
     "start_time": "2019-10-09T03:11:34.712920Z"
    }
   },
   "outputs": [],
   "source": [
    "sylls = syllable_df.audio.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:11:48.047673Z",
     "start_time": "2019-10-09T03:11:34.805841Z"
    }
   },
   "outputs": [],
   "source": [
    "nrows = 5\n",
    "ncols = 10\n",
    "zoom = 2\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows = nrows,figsize = (ncols*zoom, nrows+zoom/1.5))\n",
    "for i, syll in tqdm(enumerate(sylls), total = nrows*ncols):\n",
    "    ax = axs.flatten()[i]\n",
    "    ax.plot(syll)\n",
    "    if i == nrows*ncols -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:12:13.032286Z",
     "start_time": "2019-10-09T03:11:48.052229Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df['audio'] = [i/np.max(i) for i in tqdm(syllable_df.audio.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment isolation calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:38:05.932450Z",
     "start_time": "2019-10-09T03:38:02.789860Z"
    }
   },
   "outputs": [],
   "source": [
    "from vocalseg.dynamic_thresholding import dynamic_threshold_segmentation\n",
    "from vocalseg.dynamic_thresholding import plot_segmented_spec, plot_segmentations\n",
    "from vocalseg.utils import butter_bandpass_filter, spectrogram, int16tofloat32, plot_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:41:34.277050Z",
     "start_time": "2019-10-09T03:41:29.536934Z"
    }
   },
   "outputs": [],
   "source": [
    "len(syllable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:46:56.531289Z",
     "start_time": "2019-10-09T03:46:53.814908Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:48:31.800136Z",
     "start_time": "2019-10-09T03:48:31.625463Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({i: [np.sum(i == syllable_df.indv)] for i in syllable_df.indv.unique()}).T.sort_values(by=0, ascending=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:43:36.358311Z",
     "start_time": "2019-10-09T03:43:36.241799Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fft=1024\n",
    "hop_length_ms=.5\n",
    "win_length_ms=4\n",
    "ref_level_db=20\n",
    "pre=0.97\n",
    "min_level_db=-30\n",
    "silence_threshold = 0.1\n",
    "min_silence_for_spec=0.1\n",
    "max_vocal_for_spec=1.0,\n",
    "min_syllable_length_s = 0.01\n",
    "spectral_range = [5000, 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:00:36.869533Z",
     "start_time": "2019-10-09T03:53:53.488666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "display_results=False\n",
    "start_times = []\n",
    "end_times = []\n",
    "for idx, row in tqdm(syllable_df.iterrows(), total=len(syllable_df)):\n",
    "    results = dynamic_threshold_segmentation(\n",
    "        row.audio,\n",
    "        row.rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length_ms=hop_length_ms,\n",
    "        win_length_ms=win_length_ms,\n",
    "        ref_level_db=ref_level_db,\n",
    "        pre=pre,\n",
    "        min_level_db=min_level_db,\n",
    "        \n",
    "        silence_threshold=silence_threshold,\n",
    "        verbose=False,\n",
    "        spectral_range=spectral_range,\n",
    "        min_syllable_length_s=min_syllable_length_s,\n",
    "                min_level_db_floor=20,\n",
    "\n",
    "    )\n",
    "    \n",
    "    if results is None:\n",
    "        if display_results:\n",
    "            spec = spectrogram(\n",
    "                row.audio,\n",
    "                row.rate,\n",
    "                n_fft=n_fft,\n",
    "                hop_length_ms=hop_length_ms,\n",
    "                win_length_ms=win_length_ms,\n",
    "                ref_level_db=ref_level_db,\n",
    "                pre=pre,\n",
    "                min_level_db=min_level_db,\n",
    "            )\n",
    "            fig, ax = plt.subplots(figsize=(30,5))\n",
    "            plot_spec(spec, fig, ax);\n",
    "            plt.show()\n",
    "        start_times.append(None)\n",
    "        end_times.append(None)\n",
    "        continue\n",
    "    else:\n",
    "        if display_results:\n",
    "            plot_segmentations(\n",
    "                results[\"spec\"],\n",
    "                vocal_envelope=results[\"vocal_envelope\"],\n",
    "                onsets=results[\"onsets\"],\n",
    "                offsets=results[\"offsets\"],\n",
    "                rate=row.rate,\n",
    "                hop_length_ms=hop_length_ms,\n",
    "                figsize=(30, 5),\n",
    "                #spectral_range=spectral_range\n",
    "            )\n",
    "            plt.show()\n",
    "            \n",
    "    start_times.append(results['onsets'])\n",
    "    end_times.append(results['offsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((np.concatenate([i for i in start_times if i is not None])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T03:52:45.847610Z",
     "start_time": "2019-10-09T03:52:44.539847Z"
    }
   },
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:20:06.244799Z",
     "start_time": "2019-10-09T04:20:06.133902Z"
    }
   },
   "outputs": [],
   "source": [
    "start_times[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add start and end times to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avgn.utils.json import NoIndent, NoIndentEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:37:43.814520Z",
     "start_time": "2019-10-09T04:37:42.669093Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df['start_times'] = start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:37:55.147846Z",
     "start_time": "2019-10-09T04:37:55.066365Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df['end_times'] = end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T04:38:02.526375Z",
     "start_time": "2019-10-09T04:38:02.418646Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T16:01:59.831212Z",
     "start_time": "2019-10-09T16:01:22.621627Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, row in tqdm(syllable_df.iterrows(), total=len(syllable_df)):\n",
    "    \n",
    "    if row.start_times is None: \n",
    "        continue\n",
    "    \n",
    "    new_json = dataset.data_files[row.key].data.copy()\n",
    "    new_json['indvs'][row.indv]['elements'] = {\n",
    "        \"start_times\": NoIndent(list(row.start_times)),\n",
    "        \"end_times\": NoIndent(list(row.end_times)),\n",
    "    }\n",
    "    json_out = (\n",
    "        DATA_DIR\n",
    "        / \"processed\"\n",
    "        / (DATASET_ID + '_isolate_segmented')\n",
    "        / DT_ID\n",
    "        / \"JSON\"\n",
    "        / (row.key + \".JSON\")\n",
    "    )\n",
    "    \n",
    "    json_txt = json.dumps(new_json, cls=NoIndentEncoder, indent=2)\n",
    "    \n",
    "    ensure_dir(json_out.as_posix())\n",
    "    print(json_txt, file=open(json_out.as_posix(), \"w\"))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda01]",
   "language": "python",
   "name": "conda-env-conda01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
