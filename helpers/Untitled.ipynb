{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a322af63-0470-4701-8c2a-871bcb28f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Model\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#from keras.engine.saving import save_model, load_model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RepeatVector, K, LSTM, Lambda, np\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018 Iván de Paz Centeno\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "from tkeras import Input, Model\n",
    "#from keras.engine.saving import save_model, load_model\n",
    "from keras.layers import RepeatVector, K, LSTM, Lambda, np\n",
    "from pyfolder import PyFolder\n",
    "\n",
    "\n",
    "__author__ = \"Iván de Paz Centeno\"\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "\n",
    "class LSTMAutoencoder:\n",
    "    \"\"\"\n",
    "    The LSTM Autoencoder for dynamic timesteps series.\n",
    "    This class can be used to train an LSTM (no hidden layers yet) that behaves like an autoencoder for time series.\n",
    "    It can be fed with unfixed timesteps series.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, latent_space, input_features):\n",
    "        \"\"\"\n",
    "        Constructor of the autoencoder. Only latent space and the input features are required.\n",
    "        Note that no timesteps are required to feed this LSTM.\n",
    "\n",
    "        :param latent_space: space to compress the data to.\n",
    "        :param input_features: number of features that represent an element in the time serie.\n",
    "        \"\"\"\n",
    "\n",
    "        self._latent_space = latent_space\n",
    "        self._input_cells = input_features\n",
    "\n",
    "        self._encoder = None\n",
    "        self._decoder = None\n",
    "        self._autoencoder = None\n",
    "        self._configure_network()\n",
    "\n",
    "    def _configure_network(self):\n",
    "        \"\"\"\n",
    "        Sets up the network's layer.\n",
    "        \"\"\"\n",
    "        def repeat_vector(args):\n",
    "            [layer_to_repeat, sequence_layer] = args\n",
    "            return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "\n",
    "        encoder_input = Input(shape=(None, self._input_cells))\n",
    "        encoder_output = LSTM(self._latent_space)(encoder_input)\n",
    "\n",
    "        # Before feeding the decoder, the encoded data must be repeated as many times as time steps in the input data,\n",
    "        # but the decoder does not know beforehand how many timesteps are fed into the autoencoder.\n",
    "        # Check https://github.com/keras-team/keras/issues/7949 for the solution to this. Basically we take it\n",
    "        # dynamically from the input shape with a Lambda layer for the repeat vector.\n",
    "        # The input shape may vary per sample.\n",
    "\n",
    "        decoder_input = Lambda(repeat_vector, output_shape=(None, self._latent_space))([encoder_output, encoder_input])\n",
    "\n",
    "        decoder_output = LSTM(self._input_cells, return_sequences=True)(decoder_input)\n",
    "\n",
    "        self._autoencoder = Model(encoder_input, decoder_output)\n",
    "        self._encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "        self._autoencoder.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        Encodes the given input into a vector with the specified latent_space size.\n",
    "        :param X: a numpy array of shape [1, N, input_features]\n",
    "        :return: vector of shape [1, latent_space]\n",
    "        \"\"\"\n",
    "        return self._encoder.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Passes the specified element through the autoencoder and returns the result of the decoder.\n",
    "        :param X: a numpy array of shape [1, N, input_features]\n",
    "        :return: a numpy array of shape [1, N, input_features], that, if trained well, should be close to X.\n",
    "        \"\"\"\n",
    "        return self._autoencoder.predict(X)\n",
    "\n",
    "    def fit(self, X, epochs, ):\n",
    "        \"\"\"\n",
    "        Fits the specified data into the autoencoder.\n",
    "        :param X: a python's list containing elements, being each element a numpy array of shape [1, N, input_features].\n",
    "                  Each element can contain a different \"N\" (timesteps).\n",
    "        :param epochs: number of iterations through the whole X. On each iteration, X is shuffled.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            # We must select data from X. Like SGD.\n",
    "            # The main issue is that each element of X might have different timesteps, that's the reason to select one\n",
    "            # element randomly and apply the fit one at a time, computing epochs by ourselves\n",
    "            random.shuffle(X)\n",
    "            for element in X:\n",
    "                loss = self._autoencoder.fit(element, element, epochs=1, verbose=0).history['loss'][0]\n",
    "                losses.append(loss)\n",
    "\n",
    "            print(f\"Epoch loss: {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "    #def evaluate(self, test_X):\n",
    "    #    \"\"\"\n",
    "    #    Evaluates the autoencoder with the specified da\n",
    "    #    :param test_X:\n",
    "    #   :return:\n",
    "    #    \"\"\"\n",
    "    #    return self._autoencoder.evaluate(test_X, test_X)\n",
    "\n",
    "    def decode(self, X, timesteps):\n",
    "        \"\"\"\n",
    "        Decodes the given vector into the corresponding time series.\n",
    "        :param X: vector of shape [1, latent_space]\n",
    "        :param timesteps: number of timesteps that the time-series originally had.\n",
    "        :return: a numpy array of shape [1, timesteps, input_features]\n",
    "        \"\"\"\n",
    "        return Decoder(self._autoencoder, self._latent_space).predict(X, timesteps)\n",
    "\n",
    "    def save(self, uri):\n",
    "        \"\"\"\n",
    "        Saves the model into a given filename.\n",
    "        The model uses 4 files: one for the encoder, other for the decoder, other\n",
    "        for the autoencoder and one for the class options in JSON format.\n",
    "        :param uri: base filename.\n",
    "        \"\"\"\n",
    "        pf = PyFolder(os.path.dirname(os.path.realpath(uri)), allow_override=True)\n",
    "        pf[os.path.basename(uri)+\"_options.json\"] = {\n",
    "            'input_cells': self._input_cells,\n",
    "            'latent_space': self._latent_space,\n",
    "        }\n",
    "\n",
    "        save_model(self._autoencoder, uri+\"_lstm_autoencoder.hdf5\")\n",
    "        save_model(self._encoder, uri+\"_lstm_encoder.hdf5\")\n",
    "\n",
    "    def load(self, uri):\n",
    "        \"\"\"\n",
    "        Loads the model from the specified URI.\n",
    "        The model uses 4 files: one for the encoder, other for the decoder, other\n",
    "        for the autoencoder and one for the class options in JSON format.\n",
    "        :param uri: base filename\n",
    "        \"\"\"\n",
    "        self._encoder = load_model(uri+\"_lstm_encoder.hdf5\")\n",
    "        self._autoencoder = load_model(uri+\"_lstm_autoencoder.hdf5\")\n",
    "\n",
    "        pf = PyFolder(os.path.dirname(os.path.realpath(uri)))\n",
    "        dict_options = pf[os.path.basename(uri)+\"_options.json\"]\n",
    "\n",
    "        self._latent_space = dict_options['latent_space']\n",
    "        self._input_cells = dict_options['input_cells']\n",
    "\n",
    "    @property\n",
    "    def latent_space(self):\n",
    "        return self._latent_space\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    \"\"\"\n",
    "    Decoder dynamic class.\n",
    "    This class is required since the decoder must compute different output shape based on the number of timesteps\n",
    "    desired, which are set dynamically when decoding in the autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, autoencoder, latent_space):\n",
    "        \"\"\"\n",
    "        Constructor of the Decoder.\n",
    "        :param autoencoder: Keras model comprising the autoencoder.\n",
    "        :param latent_space: number of elements in the compressed version of the data.\n",
    "        \"\"\"\n",
    "        self._autoencoder = autoencoder\n",
    "        self._latent_space = latent_space\n",
    "\n",
    "    def predict(self, X, timesteps):\n",
    "        \"\"\"\n",
    "        Decodes the given vector into the corresponding time series.\n",
    "        :param X: vector of shape [1, latent_space]\n",
    "        :param timesteps: number of timesteps that the time-series originally had.\n",
    "        :return: a numpy array of shape [1, timesteps, input_features]\n",
    "        \"\"\"\n",
    "        decoder_input = Input(shape=(self._latent_space,))\n",
    "\n",
    "        decoder_repeated_input = RepeatVector(timesteps)(decoder_input)\n",
    "        decoder = self._autoencoder.layers[-1](decoder_repeated_input)\n",
    "        decoder = Model(decoder_input, decoder)\n",
    "\n",
    "        return decoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbfade-c7ab-4d1a-afe8-3c886a0748b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
